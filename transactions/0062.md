# BRC-62: Background Evaluation Extended Format (BEEF) Transaction

## Abstract

We propose a binary format for sending Transactions between peers to allow Simple Payment Verification (SPV). The format is optimized for minimal bandwidth while maintaining data required to independently validate the transaction in full.

Assumption: Every user has an independent source of Block Headers indexed by Merkle Root.

The simplest form is a single transaction, with extended data to allow script evaluation and satoshi amount verification, with Merkle paths for each input.

In cases where one or more inputs are not yet mined, each ancestral transaction is included. We step back through the Transaction DAG until every input has a Merkle Path or corresponding parent transaction.

Inputs with corresponding Merkle Paths will be extended with previous outpoint script and satoshis. Whereas inputs with parents included in stream are not extended since this data has already been sent.**

## Copyright

This BRC is licensed under the Open BSV license.

## Motivation

Simplified Payment Verification formats for transmitting transactions between peers has yet to see wide adoption. This proposal advocates for complete ecosystem adoption of the principles of SPV; acknowledges that the system is secured by economics incentives, and law; and lays out a binary format for transmitting the data between parties optimizing for minimal bandwidth.

Three prior formats should be mentioned:

Extended Format [BRC-30](./0030.md) incorporates the utxo script for script evaluation and satoshis for checking amounts. Ideal for confirmed txs but insufficient for unconfirmed.

[Tx Ancestors](https://tsc.bitcoinsv.com/standards/transaction-ancestors/) which was created for use within the [Lite Client Toolbox](https://docs.bitcoinsv.io), this uses an array of rawtxs, Merkle proofs, and Mapi responses to transport the data required for SPV. 

Everett-style Transaction Envelopes [BRC-8](./0008.md) uses a recursive JSON format which is otherwise similar to the above format originally designed for use within [DPP](../payments/0027.md). One of the ideas which this proposal highlights is that the target of merkle proofs could be height rather than blockhash or root, to save on bytes. Thinking along these lines, we propose that no target be provided at all, and the root simply be calculated at the receieving end as needed. Using the root to look up headers also avoids any implementation complexity associated with competing blocks. For example, when a competing block encapsulates the transactions it may not have the same height, if all transactions are being stored with paths at a height rather than blockhash or merkle root (which are unique to specific versions of a block) then updating a set of the transactions with merkle paths from the new block will be difficult to sort out. If they're indexed by blockhash then it would be trivial to set them all as "in need of updated paths" without the need for disambiguation.

Mapi is to be deprecated in the near future, so any new recommended formats should not include Mapi Responses as part of the solution.

The array of rawtxs within the Tx Ancestors spec makes some sense in that there are strange cases where the same rawtx has two outputs which are spent in different transactions, both within the ancestry of the tx we are validating. You don't want to have embedded a copy of the same proof twice, hence a hash map would make more sense than just including the merkle path bytes in the tx itself. Everett-style Transaction Envelope deals with this well even given its recursive object design. Txids are used as pointers to existing transactions when complex dependency graphs occur. This proposal uses the same thinking, but for a binary format lists are used rather than recursive objects. 

The ordering of data has streaming validation in mind - an idea raised in EF [BRC-30](./0030.md) - such that a receiver can start processing the validation immediately on receipt of the first few bytes, and any later bytes rely on previous bytes for their validation to even begin.

- Merkle Paths
- Oldest Tx Anchroed by Path
- Newer Txs depending on Oldest parent
- Newest Tx

As soon as the Merkle Paths are receieved we can calculate the roots and lookup their blockheaders. If they're not valid then validation can stop there - rejected. Then we look at the oldest ancestor - if it's valid then its children can be validated, and so on until we reach the most recent tx.

## Specification

BEEF combines thinking from several formats into one binary stream, prefixed with a header for disambiguation.

- Raw Transaction Format: [BRC-12](./0012.md)
- Extended Format (EF): [BRC-30](./0012.md)
- Compound Merkle Path Format: [BRC-61](./0061.md)

BEEF adds a marker to the transaction format. The BEEF marker allows a library that supports the format to recognize that it is dealing with a transaction in extended format, while a library that does not support extended format will read the transaction as having 0 inputs, 0 outputs and a future nLock time. This has been done to minimize the possible problems a legacy library will have when reading the extended format. It can in no way be recognized as a valid transaction. Lifted from [BRC-30](./0012.md).

| Field                | Description                                                                                            | Size                |
|----------------------|--------------------------------------------------------------------------------------------------------|---------------------|
| Version no           | Used to indicate to nodes which version of BEEF this is.                                               | 4 bytes             |
| BEEF marker          | Marker for Background Evaluation Extended Format **00000000BEEF**                                      | 6 bytes             |
| nPaths               | VarInt number of compound merkle paths to follow                                                       | 1-9 bytes           |
| Compound Merkle Path | All of the paths required to prove inclusion of inputs in longest chain of blocks [BRC-61](./0061.md)  | many bytes          |
| nTransactions        | VarInt number of transactions which follow                                                             | 1-9 bytes           |
| CEF Transaction      | RawTx bytes with input specific extended data as defined below                                         | many bytes          |


### Conditionally Extended Format (CEF) Transaction

This uses the same format detailed in EF [BRC-30](./0012.md) but only for inputs which have been mined and we have paths for. This means that the data needs to be added to some but not all inputs. Hence we will do away with the global EF flag in the data and instead use a single byte to indicate inclusion or not, For sake of clarify when reading the bytes we'll mark "extended format" with an `EF` and "not extended" as `00`.







| In-counter       | positive integer VI = [[VarInt]]                                                                       | 1 - 9 bytes                                       |
| list of inputs   | **BEEF** transaction Input Structure                                                                   | <in-counter> qty with variable length per input   |
| Out-counter      | positive integer VI = [[VarInt]]                                                                       | 1 - 9 bytes                                       |
| list of outputs  | Transaction Output Structure                                                                           | <out-counter> qty with variable length per output |
| nLocktime        | if non-zero and sequence numbers are < 0xFFFFFFFF: block height or timestamp when transaction is final | 4 bytes                                           |



RawTx standard uses the following input structure:

| Field                     | Description                                                                                 | Size        |
|---------------------------|---------------------------------------------------------------------------------------------|-------------|
| Previous Transaction hash | TXID of the transaction the output was created in                                           | 32 bytes    |
| Previous Txout-index      | Index of the output (Non negative integer)                                                  | 4 bytes     |
| Txin-script length        | Non negative integer VI = VarInt                                                            | 1 - 9 bytes |
| Txin-script / scriptSig   | Script                                                                                      | many bytes  | 
| Sequence_no               | Used to iterate inputs inside a payment channel. Input is final when nSequence = 0xFFFFFFFF | 4 bytes     |

In EF, we extend the input structure to include satoshi amount, the previous locking script, the outpoint's first 32 bytes points to a particular path associated with the txid in the path array:

| Field                          | Description                                                                                 | Size             |
|--------------------------------|---------------------------------------------------------------------------------------------|------------------|
| Previous Transaction hash      | TXID of the transaction the output was created in                                           | 32 bytes         |
| Previous Txout-index           | Index of the output (Non negative integer)                                                  | 4 bytes          |
| Txin-script length             | VarInt Unlocking Script Length                                                              | 1 - 9 bytes      |
| Txin-script / scriptSig        | Unlocking Script                                                                            | many bytes       | 
| Sequence_no                    | Used to iterate inputs inside a payment channel. Input is final when nSequence = 0xFFFFFFFF | 4 bytes          |
| **UTXO satoshi amount**        | **Output value in satoshis of previous input - uint64le**                                   | **8 bytes**      |
| **UTXO locking script length** | **VarInt length of locking script OR 00 which indicates there is a "local anchor"**         | **1 - 9 bytes**  |
| **UTXO locking script**        | **Script**                                                                                  | **many bytes**   |

The overall structure starts with the paths of all txids corresponding to their place within the blockchain. Blockhashes are not included. Instead the validator is to calculate the merkle root and lookup thier own header store by that to verify the tx appears within the valid chain of blocks.

### Local Anchor

When the VarInt following the satoshi amount of an input is "00" zero - this indicates that the input is has a local anchor. This means that the txid in the input points to a previous transactions which exists within the array of transactions in the data shared. For this reason, no further data is shared since this would be repeatition of early bytes. The process a validator would take is to at this point lookup the hashmap of validated previous transactions to determine whether to continue.

Order is important - we must ensure that we end with the tx being evaluated, and its inputs are above, and their inputs are above that. This will allow us to sequentially validate a stream of transactions as they come in without depending on data which hasn't arrived yet. For the same reason, we have to start the stream with the paths of the transactions first, prior to any tx data.
This makes the overall structure look something like:

### Overall Structure of the Transmitted Data

```javascript
version
00000000BEEF
nPaths // paths first
index
nLeaves
leaves... // repeat for each path, then ancestry starting oldest first
nTransactions // 2 in this case
// example a tx with a merkle proof anchor
  version
  0000000000EF // extended format follows
  nInputs
  outpoint lenUnlockScript unlockingScript nSequence satoshis lenLockingScript lockingScript // txid maps to the path above based on index of a Set of unique values.
  nOutputs
  outputs...
  nLocktime
// for example a tx with local parent
  version
  nInputs
  outpoint lenUnlockScript unlockingScript nSequence // parent has been processed above
  nOutputs
  outputs...
  nLocktime
```

## Backward compatibility

The Background Evaluation Extended Format is not backwards compatible, but has been designed in such a way that existing software should not read a transaction in Background Evaluation Extended Format as a valid (partial) transaction. The Background Evaluation Extended Format header (00000000BEEF) will be read as an empty transaction with a future nLock time in a library that does not support the Background Evaluation Extended Format.

## Implementation

### Transaction Order
Step one is to ensure the transactions are in topological order. Running Khan's algorithm on the transaction DAG subset would be preferred if order is ever in doubt for complex transaction chains. Example below to demonstrate with easily identifyable txids.

```javascript
// khan's algorithm
function khanTopologicalSort(graph) {
    const inDegree = {}
    const queue = []
    const result = []
    for (let node in graph) {
        inDegree[node] = 0
    }
    for (let node in graph) {
        for (let neighbor in graph[node]) {
            inDegree[neighbor]++
        }
    }
    for (let node in inDegree) {
        if (inDegree[node] === 0) {
            queue.push(node)
        }
    }
    while (queue.length) {
        let node = queue.shift()
        result.push(node)
        for (let neighbor in graph[node]) {
            inDegree[neighbor]--
            if (inDegree[neighbor] === 0) {
                queue.push(neighbor)
            }
        }
    }
    return result.reverse()
}

const txs = [
    {
        txid: '2222222222222222222222222222222222222222222222222222222222222222',
        inputs: ['1111111111111111111111111111111111111111111111111111111111111111'],
    },
    {
        txid: '1111111111111111111111111111111111111111111111111111111111111111',
        inputs: ['0000000000000000000000000000000000000000000000000000000000000000'],
    },
    {
        txid: '0000000000000000000000000000000000000000000000000000000000000000',
        inputs: [],
    },
    {
        txid: '4444444444444444444444444444444444444444444444444444444444444444',
        inputs: [
            '3333333333333333333333333333333333333333333333333333333333333333',
            '2222222222222222222222222222222222222222222222222222222222222222',
        ],
    },
    {
        txid: '3333333333333333333333333333333333333333333333333333333333333333',
        inputs: [
            '2222222222222222222222222222222222222222222222222222222222222222',
            '1111111111111111111111111111111111111111111111111111111111111111',
        ],
    },
]

const graph = {}
for (let tx of txs) {
    graph[tx.txid] = {}
    for (let input of tx.inputs) {
        graph[tx.txid][input] = true
    }
}
console.log({ graph })
console.log({ correctOrder: khanTopologicalSort(graph) })
```

### Paths

The next step of constructing a BEEF Tx would be to gather tha Merkle Path data for each input of the transaction we'd like to send. If we have the full set, then no additional transactions need be included.